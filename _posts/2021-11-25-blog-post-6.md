# Blog Post 6: Fake News Detection

For this week's blog post, we're going to be building a fake news classifier using data from "Detection of Online Fake News Using N-Gram Analysis and Machine Learning Techniques" (Ahmed et al 2017).

## §1. Acquire Training Data
We'll first read in our data using the link that Dr. Chodrow  [provided](https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true), as well as the necessary libraries.




```python
import numpy as np
import pandas as pd
import tensorflow as tf

train_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true"
df = pd.read_csv(train_url, header=0, index_col=0)

df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>title</th>
      <th>text</th>
      <th>fake</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>17366</th>
      <td>Merkel: Strong result for Austria's FPO 'big c...</td>
      <td>German Chancellor Angela Merkel said on Monday...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5634</th>
      <td>Trump says Pence will lead voter fraud panel</td>
      <td>WEST PALM BEACH, Fla.President Donald Trump sa...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>17487</th>
      <td>JUST IN: SUSPECTED LEAKER and “Close Confidant...</td>
      <td>On December 5, 2017, Circa s Sara Carter warne...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>12217</th>
      <td>Thyssenkrupp has offered help to Argentina ove...</td>
      <td>Germany s Thyssenkrupp, has offered assistance...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5535</th>
      <td>Trump say appeals court decision on travel ban...</td>
      <td>President Donald Trump on Thursday called the ...</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>



## §2. Make a Dataset

Write a function called `make_dataset`. This function should do two things:

1. Remove stopwords from the article `text` and `title`. A stopword is a word that is usually considered to be uninformative, such as “the,” “and,” or “but.” You may find this StackOverFlow thread to be helpful.

2. Construct and return a tf.data.Dataset with two inputs and one output. The input should be of the form (`title`, `text`), and the output should consist only of the fake column. You may find it helpful to consult these lecture notes or this tutorial for reference on how to construct and use Datasets with multiple inputs.

Then, I decided to batch my Dataset prior to returning it so my model could train on chunks of data rather than individual rows. This can sometimes reduce accuracy, but can also greatly increase the speed of training. Finding a balance is key. I found batches of 100 rows to work well.


```python
import nltk
from nltk.corpus import stopwords
nltk.download('stopwords') #import stopwords
stop_words = stopwords.words('english')
```

    [nltk_data] Downloading package stopwords to /root/nltk_data...
    [nltk_data]   Unzipping corpora/stopwords.zip.


After downloading our stop_words, I created a helper function to efficiently get rid of stop words. After doing this, we  can make a Dataset object! 


```python

def remove_stopwords(words):
  """
  helper function to remove stopwords used in make_dataset()
  1. split words and remove if in stopwords 2. merge back with join()
  """
  clean = [item for item in words.split() if item not in stop_words]
  return ' '.join(clean)


#make the make_dataset function
def make_dataset(df):
  """
  Makes tf.data.Dataset object from pd.Dataframe object and removes stop words for title and text

  This function also batches the Dataset object

  inspired by Prof. Chodrow's lecture notes
  """
  #df["title"] = df["title"].apply(remove_stopwords) #clean title
  #df["text"] = df["text"].apply(remove_stopwords) #clean text

  #make dataset object w/ 2 input and 1 output
  data = tf.data.Dataset.from_tensor_slices(
      (
        {
            "title" : df["title"].apply(remove_stopwords), 
            "text" : df["text"].apply(remove_stopwords)
        }, 
        {
            "fake" : df[["fake"]]
        }
      )
  )
  data = data.shuffle(buffer_size = len(data)) #shuffle as seen in Lecture Note
  #we shuffle before batching
  batched_data = data.batch(100) #Batch data

  return batched_data #return batched data
data_tf = make_dataset(df)



```

### Validation Data

We will use a 20% split for our validation data. Since we have a separate test set elsewhere, we can use the remaining 80% to our training data.

**Note: Since we already shuffled and batched our data in `make_dataset()`, then we don't need to do it here.**


```python
train_size = int(0.8*len(data_tf))
val_size   = int(0.2*len(data_tf))

train = data_tf.take(train_size)
val  = data_tf.take(val_size)
len(train), len(val)
```




    (180, 45)



### Base Rate
Recall that the base rate refers to the accuracy of a model that always makes the same guess (for example, such a model might always say “fake news!”). 

Next, we'll determine the base rate for this data set by examining the labels on the training set.


```python
#observe labels
labels_iterator= train.unbatch().map(lambda fake, label: label).as_numpy_iterator()
labels = list(labels_iterator) #iterate through list

```


```python
count = 0
for i in range(0, len(labels)): #iterate through entire list of dicts
  count+= labels[i]['fake'][0] #add number of 1 = #of fakes
print(count)
```

    9406


Because we have 9406 fakes in our classifier of a total of 18000, we will want a base rate of 0.52

## §3. Create Models

The central question for this section is:

**When detecting fake news, is it most effective to focus on only the title of the article, the full text of the article, or both?**

To address this question, we will create three (3) TensorFlow models.

1. In the first model, you should use only the article title as an input.
2. In the second model, you should use only the article text as an input.
3. In the third model, you should use both the article title and the article text as input.

First, we have some preprocessing to do seen from Week 8 Lectures! This includes `standardization()`.


```python
import re
import string

from tensorflow.keras import layers
from tensorflow.keras import losses
from tensorflow import keras

from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
from tensorflow.keras.layers.experimental.preprocessing import StringLookup

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

size_vocabulary = 2000

def standardization(input_data):
    lowercase = tf.strings.lower(input_data)
    no_punctuation = tf.strings.regex_replace(lowercase,
                                  '[%s]' % re.escape(string.punctuation),'')
    return no_punctuation 

vectorize_layer = TextVectorization(
    standardize=standardization,
    max_tokens=size_vocabulary, # only consider this many words
    output_mode='int',
    output_sequence_length=500) 

vectorize_layer.adapt(train.map(lambda x, y: x["text"]))
```


```python
# inputs

text_input = keras.Input(
    shape = (1,), 
    name = "text",
    dtype = "string"
)

title_input = keras.Input(
    shape = (1,), 
    name = "title",
    dtype = "string"
)
```

### Model 1: Text-only model

Our model will only use the text data. Moreover, we will use the **Functional Keras API** instead of Sequential.

We were also recommended to share an embedding layer for both the article title and text inputs. Therefore, we do this by defining a `shared_embedding`


```python
#text only framework
#shared embedding
shared_embedding = layers.Embedding(size_vocabulary, 3, name = "embedding")

#model constrution: inspired by lecture notes
text_features = vectorize_layer(text_input) #vectorize our features
text_features = shared_embedding(text_features) #text embedding layer
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.GlobalAveragePooling1D()(text_features)
text_features = layers.Dropout(0.2)(text_features)
text_features = layers.Dense(32, activation='relu')(text_features)
text_output = layers.Dense(1, name = "fake")(text_features) #only want 1 sub-element per element

#model declaration
text_model = keras.Model(
    inputs = [text_input],
    outputs = text_output
)

text_model.summary() #quick summary
```

    Model: "model"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     text (InputLayer)           [(None, 1)]               0         
                                                                     
     text_vectorization (TextVec  (None, 500)              0         
     torization)                                                     
                                                                     
     embedding (Embedding)       (None, 500, 3)            6000      
                                                                     
     dropout (Dropout)           (None, 500, 3)            0         
                                                                     
     global_average_pooling1d (G  (None, 3)                0         
     lobalAveragePooling1D)                                          
                                                                     
     dropout_1 (Dropout)         (None, 3)                 0         
                                                                     
     dense (Dense)               (None, 32)                128       
                                                                     
     fake (Dense)                (None, 1)                 33        
                                                                     
    =================================================================
    Total params: 6,161
    Trainable params: 6,161
    Non-trainable params: 0
    _________________________________________________________________



```python
#compile model
text_model.compile(optimizer = "adam",
              loss = losses.BinaryCrossentropy(from_logits=True),
              metrics=['accuracy']
)

history = text_model.fit(train, 
                    validation_data=val,
                    epochs = 50, 
                    verbose = False)

from matplotlib import pyplot as plt
plt.plot(history.history["accuracy"])
plt.plot(history.history["val_accuracy"])
```

    /usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:559: UserWarning: Input dict contained keys ['title'] which did not match any model input. They will be ignored by the model.
      inputs = self._flatten_to_reference_inputs(inputs)





    [<matplotlib.lines.Line2D at 0x7f45b2549110>]




    
![output_17_2.png](/images/output_17_2.png)
    


As you can see, we have great accuracy in our text-only model. Let's move onto our second model, which only uses the **titles**.

### Model 2: Title-only model


```python
#title only framework
#shared embedding
title_features = vectorize_layer(title_input)
title_features = shared_embedding(title_features)

#model constrution: inspired by lecture notes
title_features = vectorize_layer(title_input) #vectorize our features
title_features = shared_embedding(title_features) #text embedding layer
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.GlobalAveragePooling1D()(title_features)
title_features = layers.Dropout(0.2)(title_features)
title_features = layers.Dense(32, activation='relu')(title_features)
title_output = layers.Dense(1, name = "fake")(title_features) #only want 1 sub-element per element

#model declaration
title_model = keras.Model(
    inputs = [title_input],
    outputs = title_output
)

title_model.summary() #quick summary

```

    Model: "model_1"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     title (InputLayer)          [(None, 1)]               0         
                                                                     
     text_vectorization (TextVec  (None, 500)              0         
     torization)                                                     
                                                                     
     embedding (Embedding)       (None, 500, 3)            6000      
                                                                     
     dropout_2 (Dropout)         (None, 500, 3)            0         
                                                                     
     global_average_pooling1d_1   (None, 3)                0         
     (GlobalAveragePooling1D)                                        
                                                                     
     dropout_3 (Dropout)         (None, 3)                 0         
                                                                     
     dense_1 (Dense)             (None, 32)                128       
                                                                     
     fake (Dense)                (None, 1)                 33        
                                                                     
    =================================================================
    Total params: 6,161
    Trainable params: 6,161
    Non-trainable params: 0
    _________________________________________________________________



```python
#compile model
title_model.compile(optimizer = "adam",
              loss = losses.BinaryCrossentropy(from_logits=True),
              metrics=['accuracy']
)

history = title_model.fit(train, 
                    validation_data=val,
                    epochs = 50, 
                    verbose = True)

from matplotlib import pyplot as plt
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```

    Epoch 1/50


    /usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:559: UserWarning: Input dict contained keys ['text'] which did not match any model input. They will be ignored by the model.
      inputs = self._flatten_to_reference_inputs(inputs)


    180/180 [==============================] - 3s 11ms/step - loss: 0.6887 - accuracy: 0.4768 - val_loss: 0.6821 - val_accuracy: 0.4760
    Epoch 2/50
    180/180 [==============================] - 2s 11ms/step - loss: 0.6638 - accuracy: 0.4763 - val_loss: 0.6355 - val_accuracy: 0.4853
    Epoch 3/50
    180/180 [==============================] - 2s 11ms/step - loss: 0.5888 - accuracy: 0.5666 - val_loss: 0.5262 - val_accuracy: 0.6871
    Epoch 4/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.4690 - accuracy: 0.7703 - val_loss: 0.4041 - val_accuracy: 0.7749
    Epoch 5/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.3612 - accuracy: 0.8599 - val_loss: 0.3002 - val_accuracy: 0.8573
    Epoch 6/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.2884 - accuracy: 0.8952 - val_loss: 0.2399 - val_accuracy: 0.9442
    Epoch 7/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.2436 - accuracy: 0.9108 - val_loss: 0.2028 - val_accuracy: 0.9307
    Epoch 8/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.2138 - accuracy: 0.9201 - val_loss: 0.1695 - val_accuracy: 0.9371
    Epoch 9/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.1896 - accuracy: 0.9282 - val_loss: 0.1569 - val_accuracy: 0.9356
    Epoch 10/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.1757 - accuracy: 0.9314 - val_loss: 0.1436 - val_accuracy: 0.9447
    Epoch 11/50
    180/180 [==============================] - 2s 9ms/step - loss: 0.1633 - accuracy: 0.9366 - val_loss: 0.1276 - val_accuracy: 0.9580
    Epoch 12/50
    180/180 [==============================] - 2s 11ms/step - loss: 0.1517 - accuracy: 0.9432 - val_loss: 0.1210 - val_accuracy: 0.9556
    Epoch 13/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.1434 - accuracy: 0.9439 - val_loss: 0.1167 - val_accuracy: 0.9604
    Epoch 14/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.1373 - accuracy: 0.9476 - val_loss: 0.1028 - val_accuracy: 0.9653
    Epoch 15/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.1326 - accuracy: 0.9473 - val_loss: 0.1022 - val_accuracy: 0.9660
    Epoch 16/50
    180/180 [==============================] - 2s 11ms/step - loss: 0.1275 - accuracy: 0.9504 - val_loss: 0.0966 - val_accuracy: 0.9671
    Epoch 17/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.1200 - accuracy: 0.9531 - val_loss: 0.0995 - val_accuracy: 0.9604
    Epoch 18/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.1200 - accuracy: 0.9534 - val_loss: 0.0868 - val_accuracy: 0.9651
    Epoch 19/50
    180/180 [==============================] - 2s 11ms/step - loss: 0.1142 - accuracy: 0.9561 - val_loss: 0.0865 - val_accuracy: 0.9624
    Epoch 20/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.1116 - accuracy: 0.9564 - val_loss: 0.0844 - val_accuracy: 0.9718
    Epoch 21/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.1088 - accuracy: 0.9579 - val_loss: 0.0955 - val_accuracy: 0.9689
    Epoch 22/50
    180/180 [==============================] - 2s 11ms/step - loss: 0.1030 - accuracy: 0.9602 - val_loss: 0.0774 - val_accuracy: 0.9682
    Epoch 23/50
    180/180 [==============================] - 2s 11ms/step - loss: 0.1030 - accuracy: 0.9601 - val_loss: 0.0827 - val_accuracy: 0.9718
    Epoch 24/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.0994 - accuracy: 0.9613 - val_loss: 0.0837 - val_accuracy: 0.9707
    Epoch 25/50
    180/180 [==============================] - 2s 11ms/step - loss: 0.0999 - accuracy: 0.9616 - val_loss: 0.0671 - val_accuracy: 0.9780
    Epoch 26/50
    180/180 [==============================] - 2s 11ms/step - loss: 0.0972 - accuracy: 0.9623 - val_loss: 0.0752 - val_accuracy: 0.9722
    Epoch 27/50
    180/180 [==============================] - 2s 11ms/step - loss: 0.0964 - accuracy: 0.9637 - val_loss: 0.0624 - val_accuracy: 0.9764
    Epoch 28/50
    180/180 [==============================] - 2s 11ms/step - loss: 0.0936 - accuracy: 0.9626 - val_loss: 0.0755 - val_accuracy: 0.9756
    Epoch 29/50
    180/180 [==============================] - 2s 11ms/step - loss: 0.0884 - accuracy: 0.9655 - val_loss: 0.0729 - val_accuracy: 0.9720
    Epoch 30/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.0902 - accuracy: 0.9647 - val_loss: 0.0743 - val_accuracy: 0.9664
    Epoch 31/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.0897 - accuracy: 0.9651 - val_loss: 0.0630 - val_accuracy: 0.9773
    Epoch 32/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.0867 - accuracy: 0.9675 - val_loss: 0.0663 - val_accuracy: 0.9782
    Epoch 33/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.0854 - accuracy: 0.9669 - val_loss: 0.0701 - val_accuracy: 0.9702
    Epoch 34/50
    180/180 [==============================] - 2s 11ms/step - loss: 0.0861 - accuracy: 0.9660 - val_loss: 0.0628 - val_accuracy: 0.9813
    Epoch 35/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.0887 - accuracy: 0.9645 - val_loss: 0.0573 - val_accuracy: 0.9782
    Epoch 36/50
    180/180 [==============================] - 2s 11ms/step - loss: 0.0851 - accuracy: 0.9659 - val_loss: 0.0632 - val_accuracy: 0.9769
    Epoch 37/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.0824 - accuracy: 0.9678 - val_loss: 0.0568 - val_accuracy: 0.9769
    Epoch 38/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.0791 - accuracy: 0.9679 - val_loss: 0.0516 - val_accuracy: 0.9773
    Epoch 39/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.0820 - accuracy: 0.9676 - val_loss: 0.0596 - val_accuracy: 0.9820
    Epoch 40/50
    180/180 [==============================] - 2s 11ms/step - loss: 0.0801 - accuracy: 0.9686 - val_loss: 0.0502 - val_accuracy: 0.9804
    Epoch 41/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.0834 - accuracy: 0.9676 - val_loss: 0.0506 - val_accuracy: 0.9833
    Epoch 42/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.0799 - accuracy: 0.9687 - val_loss: 0.0557 - val_accuracy: 0.9807
    Epoch 43/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.0784 - accuracy: 0.9695 - val_loss: 0.0527 - val_accuracy: 0.9776
    Epoch 44/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.0790 - accuracy: 0.9695 - val_loss: 0.0616 - val_accuracy: 0.9764
    Epoch 45/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.0793 - accuracy: 0.9693 - val_loss: 0.0573 - val_accuracy: 0.9816
    Epoch 46/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.0764 - accuracy: 0.9707 - val_loss: 0.0557 - val_accuracy: 0.9816
    Epoch 47/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.0772 - accuracy: 0.9699 - val_loss: 0.0578 - val_accuracy: 0.9811
    Epoch 48/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.0785 - accuracy: 0.9698 - val_loss: 0.0581 - val_accuracy: 0.9787
    Epoch 49/50
    180/180 [==============================] - 2s 11ms/step - loss: 0.0798 - accuracy: 0.9699 - val_loss: 0.0794 - val_accuracy: 0.9784
    Epoch 50/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.0814 - accuracy: 0.9686 - val_loss: 0.0652 - val_accuracy: 0.9689





    <matplotlib.legend.Legend at 0x7f45b1d952d0>




    
![output_20_4.png](/images/output_20_4.png)
    


Great success! We got sufficient accuracy in our title-only model. Now, let's finish up our modeling and use the text-title model.

### Model 3: Text-Title combined model

We are required to make a new feature object, which we can do simply by concatenating our title with our title within a layer!

Also, we want to reduce inefficiencies. Therefore, we can use the [Functional API](https://www.tensorflow.org/guide/keras/functional) to see that we can share and reuse our features, specifically `title_features` and `text_features`. 


```python
#model constrution: inspired by lecture notes
# Merge all available features into a single large vector via concatenation
combined_features = layers.concatenate(axis= 1, [title_features, text_features]) #column-wise concatenation for our layer
combined_input = [title_input, text_input]
combined_features = layers.Dense(32, activation='relu')(combined_features)
combined_output = layers.Dense(1, name = "fake")(combined_features)
combined_model = keras.Model(
    inputs = [combined_input],
    outputs = combined_output
)
 #combined_model.summary()

```


```python
 combined_model.summary()
```

    Model: "model_2"
    __________________________________________________________________________________________________
     Layer (type)                   Output Shape         Param #     Connected to                     
    ==================================================================================================
     title (InputLayer)             [(None, 1)]          0           []                               
                                                                                                      
     text (InputLayer)              [(None, 1)]          0           []                               
                                                                                                      
     text_vectorization (TextVector  (None, 500)         0           ['text[0][0]',                   
     ization)                                                         'title[0][0]']                  
                                                                                                      
     embedding (Embedding)          (None, 500, 3)       6000        ['text_vectorization[2][0]',     
                                                                      'text_vectorization[4][0]']     
                                                                                                      
     dropout_2 (Dropout)            (None, 500, 3)       0           ['embedding[2][0]']              
                                                                                                      
     dropout (Dropout)              (None, 500, 3)       0           ['embedding[0][0]']              
                                                                                                      
     global_average_pooling1d_1 (Gl  (None, 3)           0           ['dropout_2[0][0]']              
     obalAveragePooling1D)                                                                            
                                                                                                      
     global_average_pooling1d (Glob  (None, 3)           0           ['dropout[0][0]']                
     alAveragePooling1D)                                                                              
                                                                                                      
     dropout_3 (Dropout)            (None, 3)            0           ['global_average_pooling1d_1[0][0
                                                                     ]']                              
                                                                                                      
     dropout_1 (Dropout)            (None, 3)            0           ['global_average_pooling1d[0][0]'
                                                                     ]                                
                                                                                                      
     dense_1 (Dense)                (None, 32)           128         ['dropout_3[0][0]']              
                                                                                                      
     dense (Dense)                  (None, 32)           128         ['dropout_1[0][0]']              
                                                                                                      
     concatenate (Concatenate)      (None, 64)           0           ['dense_1[0][0]',                
                                                                      'dense[0][0]']                  
                                                                                                      
     dense_2 (Dense)                (None, 32)           2080        ['concatenate[0][0]']            
                                                                                                      
     fake (Dense)                   (None, 1)            33          ['dense_2[0][0]']                
                                                                                                      
    ==================================================================================================
    Total params: 8,369
    Trainable params: 8,369
    Non-trainable params: 0
    __________________________________________________________________________________________________



```python
 combined_model.compile(optimizer = "adam",
              loss = losses.BinaryCrossentropy(from_logits=True),
              metrics=['accuracy']
)

history = title_model.fit(train, 
                    validation_data=val,
                    epochs = 50, 
                    verbose = True)

from matplotlib import pyplot as plt
plt.plot(history.history["accuracy"], label = "training")
plt.plot(history.history["val_accuracy"], label = "validation")
plt.gca().set(xlabel = "epoch", ylabel = "accuracy")
plt.legend()
```

    Epoch 1/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.0760 - accuracy: 0.9693 - val_loss: 0.0479 - val_accuracy: 0.9847
    Epoch 2/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.0776 - accuracy: 0.9689 - val_loss: 0.0606 - val_accuracy: 0.9807
    Epoch 3/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.0733 - accuracy: 0.9717 - val_loss: 0.0542 - val_accuracy: 0.9842
    Epoch 4/50
    180/180 [==============================] - 2s 9ms/step - loss: 0.0744 - accuracy: 0.9703 - val_loss: 0.0501 - val_accuracy: 0.9824
    Epoch 5/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.0761 - accuracy: 0.9697 - val_loss: 0.0557 - val_accuracy: 0.9767
    Epoch 6/50
    180/180 [==============================] - 2s 9ms/step - loss: 0.0749 - accuracy: 0.9707 - val_loss: 0.0575 - val_accuracy: 0.9749
    Epoch 7/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.0724 - accuracy: 0.9719 - val_loss: 0.0505 - val_accuracy: 0.9851
    Epoch 8/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.0734 - accuracy: 0.9709 - val_loss: 0.0503 - val_accuracy: 0.9820
    Epoch 9/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.0731 - accuracy: 0.9713 - val_loss: 0.0544 - val_accuracy: 0.9780
    Epoch 10/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.0762 - accuracy: 0.9703 - val_loss: 0.0515 - val_accuracy: 0.9800
    Epoch 11/50
    180/180 [==============================] - 2s 9ms/step - loss: 0.0725 - accuracy: 0.9715 - val_loss: 0.0499 - val_accuracy: 0.9796
    Epoch 12/50
    180/180 [==============================] - 2s 9ms/step - loss: 0.0722 - accuracy: 0.9703 - val_loss: 0.0537 - val_accuracy: 0.9827
    Epoch 13/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.0731 - accuracy: 0.9733 - val_loss: 0.0517 - val_accuracy: 0.9802
    Epoch 14/50
    180/180 [==============================] - 2s 9ms/step - loss: 0.0709 - accuracy: 0.9728 - val_loss: 0.0535 - val_accuracy: 0.9769
    Epoch 15/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.0702 - accuracy: 0.9734 - val_loss: 0.0450 - val_accuracy: 0.9836
    Epoch 16/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.0696 - accuracy: 0.9733 - val_loss: 0.0443 - val_accuracy: 0.9853
    Epoch 17/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.0686 - accuracy: 0.9731 - val_loss: 0.0473 - val_accuracy: 0.9813
    Epoch 18/50
    180/180 [==============================] - 2s 9ms/step - loss: 0.0745 - accuracy: 0.9702 - val_loss: 0.0501 - val_accuracy: 0.9851
    Epoch 19/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.0708 - accuracy: 0.9724 - val_loss: 0.0588 - val_accuracy: 0.9827
    Epoch 20/50
    180/180 [==============================] - 2s 9ms/step - loss: 0.0701 - accuracy: 0.9731 - val_loss: 0.0496 - val_accuracy: 0.9851
    Epoch 21/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.0713 - accuracy: 0.9723 - val_loss: 0.0470 - val_accuracy: 0.9833
    Epoch 22/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.0678 - accuracy: 0.9738 - val_loss: 0.0437 - val_accuracy: 0.9871
    Epoch 23/50
    180/180 [==============================] - 2s 9ms/step - loss: 0.0675 - accuracy: 0.9744 - val_loss: 0.0447 - val_accuracy: 0.9831
    Epoch 24/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.0719 - accuracy: 0.9719 - val_loss: 0.0481 - val_accuracy: 0.9838
    Epoch 25/50
    180/180 [==============================] - 2s 9ms/step - loss: 0.0683 - accuracy: 0.9731 - val_loss: 0.0461 - val_accuracy: 0.9809
    Epoch 26/50
    180/180 [==============================] - 2s 11ms/step - loss: 0.0723 - accuracy: 0.9710 - val_loss: 0.0349 - val_accuracy: 0.9871
    Epoch 27/50
    180/180 [==============================] - 2s 9ms/step - loss: 0.0690 - accuracy: 0.9732 - val_loss: 0.0583 - val_accuracy: 0.9760
    Epoch 28/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.0683 - accuracy: 0.9735 - val_loss: 0.0465 - val_accuracy: 0.9853
    Epoch 29/50
    180/180 [==============================] - 2s 11ms/step - loss: 0.0660 - accuracy: 0.9738 - val_loss: 0.0440 - val_accuracy: 0.9844
    Epoch 30/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.0685 - accuracy: 0.9720 - val_loss: 0.0394 - val_accuracy: 0.9844
    Epoch 31/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.0686 - accuracy: 0.9722 - val_loss: 0.0444 - val_accuracy: 0.9820
    Epoch 32/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.0711 - accuracy: 0.9718 - val_loss: 0.0430 - val_accuracy: 0.9867
    Epoch 33/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.0657 - accuracy: 0.9737 - val_loss: 0.0538 - val_accuracy: 0.9789
    Epoch 34/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.0672 - accuracy: 0.9730 - val_loss: 0.0482 - val_accuracy: 0.9811
    Epoch 35/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.0651 - accuracy: 0.9733 - val_loss: 0.0416 - val_accuracy: 0.9844
    Epoch 36/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.0681 - accuracy: 0.9729 - val_loss: 0.0411 - val_accuracy: 0.9896
    Epoch 37/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.0655 - accuracy: 0.9742 - val_loss: 0.0410 - val_accuracy: 0.9842
    Epoch 38/50
    180/180 [==============================] - 2s 11ms/step - loss: 0.0666 - accuracy: 0.9732 - val_loss: 0.0353 - val_accuracy: 0.9871
    Epoch 39/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.0694 - accuracy: 0.9729 - val_loss: 0.0468 - val_accuracy: 0.9816
    Epoch 40/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.0663 - accuracy: 0.9730 - val_loss: 0.0348 - val_accuracy: 0.9869
    Epoch 41/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.0664 - accuracy: 0.9732 - val_loss: 0.0447 - val_accuracy: 0.9827
    Epoch 42/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.0642 - accuracy: 0.9742 - val_loss: 0.0409 - val_accuracy: 0.9878
    Epoch 43/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.0662 - accuracy: 0.9731 - val_loss: 0.0477 - val_accuracy: 0.9840
    Epoch 44/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.0647 - accuracy: 0.9752 - val_loss: 0.0455 - val_accuracy: 0.9811
    Epoch 45/50
    180/180 [==============================] - 2s 11ms/step - loss: 0.0683 - accuracy: 0.9728 - val_loss: 0.0430 - val_accuracy: 0.9827
    Epoch 46/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.0645 - accuracy: 0.9746 - val_loss: 0.0448 - val_accuracy: 0.9833
    Epoch 47/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.0633 - accuracy: 0.9746 - val_loss: 0.0366 - val_accuracy: 0.9864
    Epoch 48/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.0662 - accuracy: 0.9731 - val_loss: 0.0348 - val_accuracy: 0.9887
    Epoch 49/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.0629 - accuracy: 0.9760 - val_loss: 0.0448 - val_accuracy: 0.9856
    Epoch 50/50
    180/180 [==============================] - 2s 10ms/step - loss: 0.0657 - accuracy: 0.9741 - val_loss: 0.0385 - val_accuracy: 0.9856





    <matplotlib.legend.Legend at 0x7f45a9dbeed0>




    
![output_24_2.png](/images/output_24_2.png)
    


We see that the combined model is very good, achieving above 97% validation accuracy. 

## §4. Model Evaluation
Now we’ll test our best model (combined) performance on unseen test data. Recall we have to make it into a `tf.data.Dataset` object, though!


```python
test_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true"
test_pandas = pd.read_csv(test_url)
test = make_dataset(test_pandas) 
```

Great, now we finished pre-processing our data! Let's run our `combined_model` for testing.

#### *Predictions on Unseen Data*

Let's check our model performance on unseen data.


```python
combined_model.evaluate(test) #evaluate model accuracy using evaluate()
```

    225/225 [==============================] - 3s 12ms/step - loss: 0.8156 - accuracy: 0.4769





    [0.8155879974365234, 0.47685864567756653]



We only get 81.6% on the test set. This may be a sign of overfitting.

**Note to grader:** To amend this, I intend to tweak this in the next coming days. For now, we will leave it as is.

## §5. Embedding Visualization

Now, we wish to visualize and comment on the embedding that your model learned.

The key question we want to answer for this part is if we are able to find any interesting patterns or associations in the words that the model found useful when distinguishing real news from fake news.

To do this, we will use principal components analysis for a 2-dimensional embedding.

*Recall that a word embedding refers to a representation of a word in a vector space. Each word is assigned an individual vector. The general aim of a word embedding is to create a representation such that words with related meanings are close to each other in a vector space, while words with different meanings are farther apart. One usually hopes for the directions connecting words to be meaningful as well.*

Let's first get weights and the vocabulary, as seen in [here](https://nbviewer.org/github/PhilChodrow/PIC16B/blob/master/lectures/tf/tf-3.ipynb).




```python
weights = combined_model.get_layer('embedding').get_weights()[0] # get the weights from the embedding layer
vocab = vectorize_layer.get_vocabulary()                # get the vocabulary from our data prep for later

weights
```




    array([[-0.00876302, -0.00856773,  0.01099402],
           [ 0.5669931 ,  0.5977117 , -0.574366  ],
           [ 0.189291  ,  0.226872  , -0.05107071],
           ...,
           [ 0.19553056,  0.28240842,  0.01737051],
           [-1.8120203 , -1.9634559 ,  1.9435182 ],
           [ 0.29390243,  0.36393252, -0.14764763]], dtype=float32)



Now, we will proceed to PCA. This becomes a dimension reduction problem, so we will reduce to 2 dimensions.


```python
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
weights = pca.fit_transform(weights)
```

Now, we'll make a dataframe from our results and then plot for visual representation.


```python
embedding_df = pd.DataFrame({
    'word' : vocab, 
    'x0'   : weights[:,0],
    'x1'   : weights[:,1]
})

import plotly.express as px 
fig = px.scatter(embedding_df, 
                 x = "x0", 
                 y = "x1", 
                 size = list(np.ones(len(embedding_df))),
                 size_max = 2,
                 hover_name = "word")

fig.show() 
```
![pca_1.png](/images/pca_1.png)



```python
#!pip install plotly>=4.0.0
#!wget https://github.com/plotly/orca/releases/download/v1.2.1/orca-1.2.1-x86_64.AppImage -O /usr/local/bin/orca
#!chmod +x /usr/local/bin/orca
#!apt-get install xvfb libgtk2.0-0 libgconf-2-4
fig.write_image("pca_1.png")
from google.colab import files
files.download('pca_1.png')
```


    <IPython.core.display.Javascript object>



    <IPython.core.display.Javascript object>


### Commentary on Words

We wish to comment on 5 words that can make our embedding interpretable. We can do this by looking for any bias in our model. In this case, we can look at the balance of "femininity" and "masculinity" in our model.


```python
feminine = ["she", "her", "woman"]
masculine = ["he", "him", "man"]

highlight_1 = ["strong", "powerful", "smart",     "thinking"]
highlight_2 = ["hot",    "sexy",     "beautiful", "shopping"]

def gender_mapper(x):
    if x in feminine:
        return 1
    elif x in masculine:
        return 4
    elif x in highlight_1:
        return 3
    elif x in highlight_2:
        return 2
    else:
        return 0

embedding_df["highlight"] = embedding_df["word"].apply(gender_mapper)
embedding_df["size"]      = np.array(1.0 + 50*(embedding_df["highlight"] > 0))
import plotly.express as px 

fig = px.scatter(embedding_df, 
                 x = "x0", 
                 y = "x1", 
                 color = "highlight",
                 size = list(embedding_df["size"]),
                 size_max = 10,
                 hover_name = "word")

fig.show()
```
![pca_2.png](/images/pca_2.png)



We see that the words "powerful" and "strong" are pretty far apart from "she," "woman," and "her."

This is quite indicative of telling us that our model is biased and the results present this finding.

In this case, our model needs further fixing in the future.
