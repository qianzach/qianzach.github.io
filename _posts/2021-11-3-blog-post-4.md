# Blog Post: Spectral Clustering

Hi there! For this week, we will be learning about scientific computing and linear algebra. We will use these skills and apply them to spectral clustering!

### Remarks on Notation

In all the math below: 

- Boldface capital letters like $\mathbf{A}$ refer to matrices (2d arrays of numbers). 
- Boldface lowercase letters like $\mathbf{v}$ refer to vectors (1d arrays of numbers). 
- $\mathbf{A}\mathbf{B}$ refers to a matrix-matrix product (`A@B`). $\mathbf{A}\mathbf{v}$ refers to a matrix-vector product (`A@v`). 

Aside from the notation that Professor Chodrow uses, I also use some basic math notation (i.e. $\forall, \in, \exists$, etc.)

I will also be showing the mathematical formulations of these algorithms as well for the sake of my enjoyment. If I'm being honest, I found the mathematical definitions a lot easier to understand in terms of knowing what to do.

## Introduction

In this problem, we'll study *spectral clustering*. Spectral clustering is an important tool for identifying meaningful parts of data sets with complex structure.

Let's look at an example. First, let's load in our libraries


```python
#Load Libraries
import numpy as np
from sklearn import datasets
from matplotlib import pyplot as plt
```

We'll make some clusters with some of the code provided by Professor Chodrow


```python
n = 200
np.random.seed(1111)
X, y = datasets.make_blobs(n_samples=n, shuffle=True, random_state=None, centers = 2, cluster_std = 2.0)
plt.scatter(X[:,0], X[:,1])
```




    <matplotlib.collections.PathCollection at 0x7fc40e92c550>




    
![png](output_3_1.png)
    


In mathematical contexts, *clustering* refers to the task of separating this data set into the two natural partitions. It is particularly useful in mathematical areas, such as networks. It's also studied for unsupervised machine learning techniques. For example K-means clustering is a very common way to achieve this task, which has good performance on circular-ish blobs like these: 


```python
from sklearn.cluster import KMeans
km = KMeans(n_clusters = 2)
km.fit(X)

plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```




    <matplotlib.collections.PathCollection at 0x7fc410033690>




    
![png](output_5_1.png)
    


However, what would we do if we had a geometric assumption to our data? Let's have the example of two crescent moons. We see that there are two clear clusters, but it's still not as defined as the prior toy example:


```python
np.random.seed(1234)
n = 200
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)
plt.scatter(X[:,0], X[:,1])
```




    <matplotlib.collections.PathCollection at 0x7fc410033610>




    
![png](output_7_1.png)
    



```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```




    <matplotlib.collections.PathCollection at 0x7fc41010e990>




    
![png](output_8_1.png)
    


A-ha! So K-Means clustering doesn't work as well as expected. 

Let's try and solve this with **spectral clustering**.

## Part A

First, we will construct a *similarity matrix* $\mathbf{A}$. $\mathbf{A}$ should be a matrix with shape `(n, n)` with `n` being our observed sample size of data points. 

When constructing the similarity matrix, we use a parameter `epsilon`. Entry `A[i,j]` should be equal to `1` if `X[i]` (the coordinates of data point `i`) is within distance `epsilon` of `X[j]` (the coordinates of data point `j`), and `0` otherwise. 

**The diagonal entries `A[i,i]` should all be equal to zero.** The function `np.fill_diagonal()` is a good way to set the values of the diagonal of a matrix.  

#### Professor's Note

It is possible to do this manually in a `for`-loop, by testing whether `(X[i] - X[j])**2 < epsilon**2` for each choice of `i` and `j`. This is not recommended! Instead, see if you can find a solution built into `sklearn`. Can you find a function that will compute all the pairwise distances and collect them into an appropriate matrix for you? 

**For this part, we'll use `epsilon = 0.4`.**

**Moreover, we will work with `scipy`'s `spatial.distance` function. Specifically, we use `pdist()` and `squareform()`. **

`pdist()` computes the distance between $m$ different points with a specified metric. Since we are in a Cartesian plane, we'll stick with the standard Euclidean distance.

Next, `squareform()` helps make our values into a square matrix!


```python
n = 200
epsilon =  0.4
from scipy.spatial.distance import pdist,squareform


A = squareform(pdist(X, metric='euclidean'))
A = np.where((A < epsilon), 1, 0)
np.fill_diagonal(A, 0)
```

## Part B

Great! Now we have our similarity matrix. 

The matrix `A` now contains information about which points are near (within distance `epsilon`) which other points. Our next immediate task is to cluster the data points in `X` as the task of partitioning the rows and columns of `A`. 

#### Mathematical Intuition
Let $d_i = \sum_{j = 1}^n a_{ij}$ be the $i$th row-sum of $\mathbf{A}$, which is also called the *degree* of $i$. Let $C_0$ and $C_1$ be two clusters of the data points. We assume that every data point is in either $C_0$ or $C_1$. The cluster membership as being specified by `y`. We think of `y[i]` as being the label of point `i`. So, if `y[i] = 1`, then point `i` (and therefore row $i$ of $\mathbf{A}$) is an element of cluster $C_1$.  

The *binary norm cut objective* of a matrix $\mathbf{A}$ is the function 

$$N_{\mathbf{A}}(C_0, C_1)\equiv \mathbf{cut}(C_0, C_1)\left(\frac{1}{\mathbf{vol}(C_0)} + \frac{1}{\mathbf{vol}(C_1)}\right)\;.$$

In this expression, 
- $\mathbf{cut}(C_0, C_1) \equiv \sum_{i \in C_0, j \in C_1} a_{ij}$ is the *cut* of the clusters $C_0$ and $C_1$. 
- $\mathbf{vol}(C_0) \equiv \sum_{i \in C_0}d_i$, where $d_i = \sum_{j = 1}^n a_{ij}$ is the *degree* of row $i$ (the total number of all other rows related to row $i$ through $A$). The *volume* of cluster $C_0$ is a measure of the size of the cluster. 

A pair of clusters $C_0$ and $C_1$ is considered to be a "good" partition of the data when $N_{\mathbf{A}}(C_0, C_1)$ is small. To see why, let's look at each of the two factors in this objective function separately. 


#### B.1 The Cut Term

First, the cut term $\mathbf{cut}(C_0, C_1)$ is the number of nonzero entries in $\mathbf{A}$ that relate points in cluster $C_0$ to points in cluster $C_1$. Saying that this term should be small is the same as saying that points in $C_0$ shouldn't usually be very close to points in $C_1$. 

Write a function called `cut(A,y)` to compute the cut term. You can compute it by summing up the entries `A[i,j]` for each pair of points `(i,j)` in different clusters. 

For `cut()`, we iterate in $O(n^2)$ complexity and check if the labels are equal or not. If they are not (ie different clusters), then we sum up the row entry.

We return a scalar *total*$ \in \mathbb{R}$.


```python
def cut(A, y):
    """
    compute cut term by summing up entries A[i,j] for each pair of points (i,j) in different clusters
    """
    total = 0
    #we want to loop through all the possible times where y_i != y_j 
    #therefore, we can do this exhaustively in O(n^2) time
    for j in range(0, len(y)):
        for k in range(0, len(y)):
            if y[j] != y[k]: #if the two points are in different clusters
                total+= A[j,k] #sum the row entry to our sum
    return total
```

Now, we compute the cut objective for the true clusters `y`. 

To compare with our result, we generate a random vector of random labels of length `n`, with each label equal to either 0 or 1. 

We expect that the cut objective for the true labels is *much* smaller than the cut objective for the random labels. 

This shows that this part of the cut objective indeed favors the true clusters over the random ones. 


```python
print(cut(A,y))

rand_labels = np.random.randint(0,2,n) #create random vector of {0, 1} for length n
print(cut(A, rand_labels))
```

    26
    2232


#### B.2 The Volume Term 

Now, we compute the *volume* algorithm. As mentioned above, the *volume* of cluster $C_0$ is a measure of how "big" cluster $C_0$ is. If we choose cluster $C_0$ to be small, then $\mathbf{vol}(C_0)$ will be small and $\frac{1}{\mathbf{vol}(C_0)}$ will be large, leading to an undesirable higher objective value. 

Synthesizing, the binary normcut objective asks us to find clusters $C_0$ and $C_1$ such that:

1. There are relatively few entries of $\mathbf{A}$ that join $C_0$ and $C_1$. 
2. Neither $C_0$ and $C_1$ are too small. 

Now, I wrote a function called `vols(A,y)` which computes the volumes of $C_0$ and $C_1$, returning them as a tuple (ie `v0, v1 = vols(A,y)` should result in `v0` holding the volume of cluster `0` and `v1` holding the volume of cluster `1`).

After this, we write the complete norm-cut objective function, called `normcut(A,y)` which uses `cut(A,y)` and `vols(A,y)` to compute the binary normalized cut objective of a matrix `A` with clustering vector `y`. 


```python
def vols(A, y):
    """
    computes volumes of C_0 and C_1
    Note that this is a function specific to a binary case
    """

    v0, v1 = np.sum(A[y== 0]) ,np.sum(A[y==1]) 
    
    return (v0, v1)

def normcut(A, y):
    vc = vols(A,y)
    normcut_obj = cut(A,y)*(1/vc[0] + (1/vc[1]))
    return(normcut_obj) 
    

```

Now, we compare the `normcut` objective using both the true labels `y` and the fake labels you generated above.


```python
print(normcut(A,y))
print(normcut(A, rand_labels))
```

**Answer:** I noticed that the normcut value of the non-randomly generated labels `y` is much less than the value using the randomly generated labels.

This makes sense, as we would expect less error in non-trivially generated values with our data.

## Part C

Great! We have now defined a normalized cut objective which takes small values when the input clusters are (a) joined by relatively few entries in $A$ and (b) not too small. One approach to clustering is to try to find a cluster vector `y` such that `normcut(A,y)` is small. However, this is an NP-hard combinatorial optimization problem, which means that may not be possible to find the best clustering in practical time, even for relatively small data sets. 

Professor Chodrow presents a mathematical approach to make this algorithm computationally feasible. 

Here's the trick: define a new vector $\mathbf{z} \in \mathbb{R}^n$ such that: 

$$
z_i = 
\begin{cases}
    \frac{1}{\mathbf{vol}(C_0)} &\quad \text{if } y_i = 0 \\ 
    -\frac{1}{\mathbf{vol}(C_1)} &\quad \text{if } y_i = 1 \\ 
\end{cases}
$$


Note that the signs of  the elements of $\mathbf{z}$ contain all the information from $\mathbf{y}$: if $i$ is in cluster $C_0$, then $y_i = 0$ and $z_i > 0$. 

We then show that:

$$\mathbf{N}_{\mathbf{A}}(C_0, C_1) = \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}}\;,$$

where $\mathbf{D}$ is the diagonal matrix with nonzero entries $d_{ii} = d_i$, and  where $d_i = \sum_{j = 1}^n a_i$ is the degree (row-sum) from before.  

1. In this part, we write a function called `transform(A,y)` to compute the appropriate $\mathbf{z}$ vector given `A` and `y`, using the formula above. 
2. Then, we have to check the equation above that relates the matrix product to the normcut objective, by computing each side separately and checking that they are equal. 
3. Lastly, we validate our results by showing the identity $\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$, where $\mathbb{1}$ is the vector of `n` ones (i.e. `np.ones(n)`). This identity effectively says that $\mathbf{z}$ should contain roughly as many positive as negative entries. 

#### Programming Note

You can compute $\mathbf{z}^T\mathbf{D}\mathbf{z}$ as `z@D@z`, provided that you have constructed these objects correctly. 



```python
def transform(A,y):
    """
    compute the z vector (indicator fxn)
    """
    vc = vols(A, y)
    z = np.where(y ==0, 1/vc[0],-1/vc[1])
    return z


z =  transform(A,y)
diag_entries = np.sum(A, axis = 1)
D = np.diag(diag_entries)
normcut_2  =(z@(D-A)@z)/ (z@D@z)
print(normcut(A,y))
print(normcut_2)
```

    0.02303682466323045
    0.011518412331615133


We see that it is marginally off. Not bad!

To check our implementation, we want to verify $\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$. We must cast our result into an `int` due to pythonic rounding.


```python
int(z@D@np.ones(n))
```




    0



### **Pog moment.**

## Part D

In Part C, we saw that the problem of minimizing the normcut objective is mathematically related to the problem of minimizing the function 

$$ R_\mathbf{A}(\mathbf{z})\equiv \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}} $$

subject to the condition $\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$. 

Turns out, it's actually possible to bake this condition into the optimization, by substituting for $\mathbf{z}$ the orthogonal complement of $\mathbf{z}$ relative to $\mathbf{D}\mathbf{1}$. In the code below, I define an `orth_obj` function which handles this for you. 

We can use the `minimize` function from `scipy.optimize` to minimize the function `orth_obj` with respect to $\mathbf{z}$. 

We then name the minimizing vector a name `z_`. 


```python
#using Prof. Chodrow's functions
def orth(u, v):
    return (u @ v) / (v @ v) * v

e = np.ones(n) 

d = D @ e

def orth_obj(z):
    z_o = z - orth(z, d)
    return (z_o @ (D - A) @ z_o)/(z_o @ D @ z_o)

from scipy import optimize
z_ = optimize.minimize(orth_obj, z)
z_
```




          fun: 0.005386460740330007
     hess_inv: array([[ 0.05471927,  0.05205312,  0.04797743, ...,  0.05912472,
             0.05702664,  0.04331411],
           [ 0.05205312,  0.90818034,  0.05160147, ...,  0.08988184,
             0.08995809,  0.05811235],
           [ 0.04797743,  0.05160147,  0.67226929, ...,  0.08424507,
             0.0673593 ,  0.05575617],
           ...,
           [ 0.05912472,  0.08988184,  0.08424507, ...,  0.75584906,
             0.0159814 , -0.18296377],
           [ 0.05702664,  0.08995809,  0.0673593 , ...,  0.0159814 ,
             0.19230123,  0.0407089 ],
           [ 0.04331411,  0.05811235,  0.05575617, ..., -0.18296377,
             0.0407089 ,  0.57341807]])
          jac: array([ 2.80317264e-02, -1.55609951e-03, -1.91569278e-02,  1.19180645e-01,
            5.59893332e-03, -4.52002718e-02,  1.67546812e-02, -5.62763226e-03,
           -1.86690454e-02, -8.93651531e-03, -1.75575656e-02,  8.84842599e-03,
           -1.19304627e-02, -1.45196068e-02,  2.42737923e-02, -4.36233461e-03,
            1.57345465e-03, -5.86223387e-03, -1.88365486e-02,  7.82222953e-04,
            1.44201971e-02, -3.61426114e-02,  2.20359127e-02, -9.21559910e-03,
           -5.23405330e-03,  8.07999505e-03, -3.74703342e-03, -1.85627941e-02,
            3.22647003e-03, -2.92035028e-02,  1.95969642e-02,  5.02646045e-03,
           -9.58949147e-03, -2.89211979e-02,  2.32042448e-02,  4.69874113e-03,
           -5.23412199e-03, -1.18657919e-02, -2.94881980e-02, -3.60533778e-02,
            2.65845481e-02,  8.09178594e-03, -3.29835291e-02,  4.41772856e-02,
            5.35674440e-03, -2.97422808e-02,  8.37051083e-03,  1.34138877e-02,
            2.74110431e-02, -9.16903742e-02, -1.64705125e-03, -1.04776950e-02,
            2.20359461e-02, -2.52455780e-02, -1.90502477e-02, -9.49303020e-03,
           -1.40685485e-02,  1.57872453e-02, -4.12182438e-02, -3.28611245e-02,
            1.84822888e-02,  2.05246412e-02,  8.63698538e-03, -3.99266683e-02,
            1.63331187e-02, -1.57160785e-02,  1.84527027e-02, -1.55469810e-02,
           -1.77399831e-02,  1.15279342e-02,  1.95315865e-02,  8.65039421e-03,
           -7.49192736e-03,  1.63188346e-02, -2.48871854e-02,  1.17033821e-01,
            1.52760495e-02, -9.96564928e-03, -1.41271076e-02, -1.62013627e-02,
            4.24900165e-02,  3.82761296e-03, -2.78667477e-03, -1.07468212e-02,
            1.01550582e-02, -2.33112114e-02, -2.89211872e-02, -1.51013652e-02,
            1.39104670e-02, -7.02504027e-02,  2.07766148e-02,  8.63700826e-03,
            1.63063848e-02, -1.64707808e-03,  1.57878913e-02, -8.93653644e-03,
           -1.18657944e-02, -1.48929181e-02, -7.69655569e-03, -1.04776997e-02,
           -6.15436077e-03,  2.74110296e-02, -6.49546512e-03, -5.87694172e-04,
           -2.77196284e-03, -9.49906028e-03,  3.36001380e-02, -2.17950133e-02,
           -1.39967004e-02, -1.49836997e-04,  1.53626799e-02,  1.39899843e-02,
            1.80008320e-02,  7.94822426e-03,  1.57868150e-02,  6.21391690e-02,
            8.82517546e-04,  1.66983127e-02,  1.57848048e-02,  1.39276532e-03,
           -8.53056466e-03, -2.31364020e-02,  2.74701814e-02, -1.03300553e-02,
            7.57101225e-06, -1.89917297e-02, -8.26429587e-03,  1.66207880e-02,
            5.02680277e-03,  1.57941718e-02, -1.26717515e-02,  1.67264360e-02,
           -3.28610330e-02, -3.22811289e-02,  1.43320357e-02,  3.35355732e-03,
            3.13171630e-02, -2.56264035e-02,  1.49039337e-02, -1.98426839e-02,
            2.15355257e-02, -9.53387714e-03, -4.84349356e-02,  2.83208271e-02,
           -4.42257174e-03, -1.20223941e-02,  7.31141539e-04,  3.34816625e-02,
           -2.17377584e-02, -1.41297798e-02, -3.29835396e-02, -1.26717148e-02,
           -2.48872135e-02,  8.07998667e-03,  2.57981454e-02,  1.83828274e-02,
            3.44340721e-02, -1.04776575e-02, -1.20224187e-02, -2.78667262e-03,
            7.79470906e-03,  2.18263827e-04,  4.13440983e-03, -1.51013153e-02,
            1.09356433e-02,  8.07998615e-03, -9.49906424e-03,  2.83208002e-02,
            1.67918373e-02,  2.36205715e-02, -4.38475033e-03,  6.90736127e-03,
           -1.49855623e-04, -2.50036735e-03, -2.33112212e-02, -7.24404008e-03,
            1.28506899e-02,  3.34815941e-02,  3.51249898e-02, -1.03300529e-02,
            1.57344004e-03, -4.44567890e-03,  2.05246608e-02, -8.34061985e-03,
           -1.86690162e-02, -7.24427064e-03, -4.01431520e-04,  2.32042621e-02,
           -1.40685294e-02,  2.74110396e-02, -7.24393927e-03,  7.79469940e-03,
           -4.01451427e-04,  1.97697582e-02, -4.18898423e-03, -4.36239206e-03,
            1.14874095e-02, -4.58229099e-02,  8.62941348e-02, -3.32702095e-02])
      message: 'Desired error not necessarily achieved due to precision loss.'
         nfev: 32975
          nit: 74
         njev: 164
       status: 2
      success: False
            x: array([-1.83362342e-03, -2.41708430e-03, -1.20224742e-03, -1.41756099e-03,
           -9.85628331e-04, -1.23088688e-03, -5.85677788e-04, -8.67498832e-04,
           -2.13361625e-03, -1.82273462e-03, -2.18408823e-03, -1.18180287e-03,
           -1.12572149e-03, -2.27571810e-03, -2.49843141e-03, -2.32675780e-03,
           -2.03867836e-03, -1.27313188e-03, -1.24223973e-03, -1.14286124e-03,
           -2.24035040e-03, -1.12830045e-03, -2.25939166e-03, -1.41895582e-03,
           -8.52852654e-04, -1.94377572e-03, -1.07879434e-03, -2.00283776e-03,
           -2.16756602e-03, -1.20011587e-03, -1.00745659e-03, -2.04781803e-03,
           -2.38493132e-03, -2.15586626e-03, -2.15344478e-03, -2.30816404e-03,
           -8.52852657e-04, -2.46826897e-03, -2.33962215e-03, -1.16168927e-03,
           -2.22252156e-03, -1.27171641e-03, -1.12820547e-03, -3.46839109e-04,
           -9.25869682e-04, -1.41518656e-03, -1.27239536e-03, -2.31477496e-03,
           -2.35163450e-03, -1.11529085e-03, -1.13552822e-03, -1.14960005e-03,
           -2.25939166e-03, -2.21233640e-03, -7.09451502e-04, -1.15743262e-03,
           -2.38251990e-03, -1.43649482e-03, -2.25740507e-03, -1.34602272e-03,
           -4.20122719e-04, -1.11090242e-03, -2.05790849e-03, -1.25854261e-03,
           -2.04523820e-03, -1.08286672e-03, -4.20124112e-04, -8.54347590e-04,
           -1.42140951e-03, -2.33031525e-03, -2.03788076e-03, -2.38894934e-03,
           -2.18989185e-03, -1.34298397e-03, -1.14331548e-03, -1.65979265e-03,
           -1.93559632e-03, -1.27236303e-03, -2.39844064e-03, -1.09191926e-03,
           -1.44541487e-03, -1.05104918e-03, -1.22163888e-03, -2.26389514e-03,
           -2.25746712e-03, -2.33474242e-03, -2.15586626e-03, -1.35430514e-03,
           -1.13226689e-03, -1.27209455e-03, -7.25758678e-04, -2.05790849e-03,
           -1.17091486e-03, -1.13552822e-03, -1.43649479e-03, -1.82273462e-03,
           -2.46826897e-03, -2.40311326e-03, -9.53941799e-04, -1.14960005e-03,
           -2.47020958e-03, -2.35163450e-03, -2.31356445e-03, -6.55844685e-04,
           -2.22991237e-03, -2.33806470e-03, -9.04965828e-04, -1.34572802e-03,
           -2.35518081e-03, -2.26938269e-03, -9.81120411e-04, -1.23223518e-03,
           -1.20421502e-03, -2.33198442e-03, -1.12900591e-03, -7.78434375e-04,
            8.23520065e-05, -5.85680442e-04, -1.43649493e-03, -2.39009167e-03,
           -2.60603439e-03, -1.20841159e-03, -2.22125874e-03, -2.30545312e-03,
           -2.11886011e-03, -1.15349678e-03, -2.37701133e-03, -1.35923792e-03,
           -2.04781802e-03, -1.43649451e-03, -1.00401979e-03, -5.85679118e-04,
           -1.34602272e-03, -1.07244356e-03, -2.38604554e-03, -2.47180870e-03,
           -2.32141569e-03, -2.07607741e-03, -2.22255395e-03, -1.55195909e-03,
           -1.84562623e-03, -2.37809386e-03, -1.21764440e-03, -1.27594963e-03,
           -1.62201568e-03, -1.21530896e-03, -1.03800334e-03, -1.23444899e-03,
           -1.12847342e-03, -1.20801663e-03, -1.12820547e-03, -1.00401979e-03,
           -1.14331549e-03, -1.94377572e-03, -2.50548070e-03, -4.20127403e-04,
           -1.97569309e-03, -1.14960005e-03, -1.21530896e-03, -1.22163888e-03,
           -2.42207339e-03, -2.24462924e-03, -1.08618879e-03, -1.35430514e-03,
           -1.51459793e-03, -1.94377572e-03, -2.33806470e-03, -1.27594963e-03,
           -5.85676038e-04, -4.45266436e-04, -2.53380516e-03, -1.16190556e-03,
           -2.26938269e-03, -2.32107671e-03, -2.33474242e-03, -1.20103167e-03,
           -1.20031936e-03, -1.23444899e-03, -1.41548762e-03, -2.30545312e-03,
           -2.03867836e-03, -1.15082358e-03, -1.11090242e-03, -1.99746517e-03,
           -2.13361625e-03, -2.05047233e-03, -9.86875821e-04, -2.15344478e-03,
           -2.38251990e-03, -2.35163450e-03, -1.20103167e-03, -2.42207339e-03,
           -9.86875822e-04, -2.52234082e-03, -2.27502655e-03, -2.32675781e-03,
           -2.09438363e-03, -1.33507080e-03, -1.49827032e-03, -1.33643824e-03])



## Part E

Recall that, by design, only the sign of `z_min[i]` actually contains information about the cluster label of data point `i`. Plot the original data, using one color for points such that `z_min[i] < 0` and another color for points such that `z_min[i] >= 0`. 

Unfortunately, it doesn't look like we did that good of a job clustering. 


```python
from matplotlib import pyplot as plt
import seaborn as sns

sns.set() #set as default
colorKey= np.where(z_.x < 0, "cornflowerblue", "fuchsia")

plt.scatter(X[:,0], X[:,1], c = colorKey)
```




    <matplotlib.collections.PathCollection at 0x7fc4112a2fd0>




    
![png](output_29_1.png)
    


## Part F: Optimizing Time-Space Complexity

Explicitly optimizing the orthogonal objective is  *way* too slow to be practical. If spectral clustering required that we do this each time, no one would use it. 

The reason that spectral clustering actually matters, and indeed the reason that spectral clustering is called *spectral* clustering, is that we can actually solve the problem from Part E using eigenvalues and eigenvectors of matrices. 

Recall that what we would like to do is minimize the function 

$$ R_\mathbf{A}(\mathbf{z})\equiv \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}} $$

with respect to $\mathbf{z}$, subject to the condition $\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$. 

The Rayleigh-Ritz Theorem states that the minimizing $\mathbf{z}$ must be the solution with smallest eigenvalue of the generalized eigenvalue problem 

$$ (\mathbf{D} - \mathbf{A}) \mathbf{z} = \lambda \mathbf{D}\mathbf{z}\;, \quad \mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$

which is equivalent to the standard eigenvalue problem 

$$ \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A}) \mathbf{z} = \lambda \mathbf{z}\;, \quad \mathbf{z}^T\mathbb{1} = 0\;.$$

This is very helpful because $\mathbb{1}$ is actually the eigenvector with smallest eigenvalue of the matrix $\mathbf{D}^{-1}(\mathbf{D} - \mathbf{A})$. 

> So, the vector $\mathbf{z}$ that we want must be the eigenvector with  the *second*-smallest eigenvalue. 

For this, we'll construct the matrix $\mathbf{L} = \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A})$, which is often called the (normalized) *Laplacian* matrix of the similarity matrix $\mathbf{A}$.

We will then find the eigenvector corresponding to its second-smallest eigenvalue, and call it `z_eig`. 

Lastly, we will plot the data again, using the sign of `z_eig` as the color as validation. 

In fact, `z_eig` should be proportional to `z_min`, although this won't be exact because minimization has limited precision by default. 

We can use Professor Chodrow's code in lecture notes.


```python
L = np.linalg.inv(D)@(D-A)
Lam, U = np.linalg.eig(L)


ix = Lam.argsort()
Lam, U = Lam[ix], U[:,ix]

# 2nd smallest eigenvalue and corresponding eigenvector
Lam[1], U[:,1]
z_eig  = U[:,1]
colorKey= np.where(z_eig < 0, "cornflowerblue", "fuchsia")
plt.scatter(X[:,0], X[:,1], c = colorKey)
```




    <matplotlib.collections.PathCollection at 0x7fc4112a2f50>




    
![png](output_31_1.png)
    


We see that we did a **much** better job clustering!

## Part G

Now, we'll put it all together. We'll write a function called `spectral_clustering(X, epsilon)` which takes in the input data `X` (in the same format as Part A) and the distance threshold `epsilon` and performs spectral clustering, returning an array of binary labels indicating whether data point `i` is in group `0` or group `1`. 

#### Outline

Our steps are as follows:

1. Construct the similarity matrix. 
2. Construct the Laplacian matrix. 
3. Compute the eigenvector with second-smallest eigenvalue of the Laplacian matrix. 
4. Return labels based on this eigenvector. 


```python
def spectral_clustering(X, epsilon):
    """
    compiles everything together
    first construct the similarity matrix
    
    then construct our diagonal matrix and then use it to compute our Laplacian matrix L = D^-1 (D-A)
    
    Lastly, we return labels based on the eigenvector we compute stemming from the second smallest eigenvalue
    """
    A = squareform(pdist(X, metric='euclidean')) #find the euclidean distances
    A = np.where((A < epsilon), 1, 0) #change thresholds
    np.fill_diagonal(A, 0) #fix diagonals
    
    D = np.diag(np.sum(A, axis = 1))
    L = np.linalg.inv(D)@(D-A)
    Lam, U = np.linalg.eig(L)

    
    ix = Lam.argsort()
    Lam, U = Lam[ix], U[:,ix]
    z_eig  = U[:,1]
    
    return np.where(z_eig < 0, "cornflowerblue", "fuchsia") #i just love cornflowerblue lol
    
    
```

## Part H

We can then run a few experiments using your function, by generating different data sets using `make_moons`.

What happens when you increase the `noise`? Does spectral clustering still find the two half-moon clusters?

For these experiments, it was useful to increase `n` to `1000` or so -- we can do this now, because of our fast algorithm! 


```python
#minimal noise:
np.random.seed(420)
n = 1000
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)
epsilon = 0.4
labs = spectral_clustering(X, epsilon)
plt.scatter(X[:,0], X[:,1], c=labs)
```




    <matplotlib.collections.PathCollection at 0x7fc412d74ed0>




    
![png](output_35_1.png)
    



```python
#more noise:
np.random.seed(420)
n = 1000
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.25, random_state=None)
epsilon = 0.4
labs = spectral_clustering(X, epsilon)
plt.scatter(X[:,0], X[:,1], c=labs)
```




    <matplotlib.collections.PathCollection at 0x7fc41370dd50>




    
![png](output_36_1.png)
    


We see increasing the nosie makes it less crescent shaped, but we still get good clustering!

## Part I

Now we try with concentric circles. Do we get good acccuracy? Yep, you bet.


```python
np.random.seed(420)
n = 1000
X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)
circ = spectral_clustering(X, epsilon = 0.4)
plt.scatter(X[:,0], X[:,1], c=circ)
```




    <matplotlib.collections.PathCollection at 0x7fc3f81f5650>




    
![png](output_38_1.png)
    



```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```




    <matplotlib.collections.PathCollection at 0x7fc4138cfe10>




    
![png](output_39_1.png)
    


Observe that K-Means clustering doesn't do well,yet our function does. Notta big deal!

Can your function successfully separate the two circles? Some experimentation here with the value of `epsilon` is likely to be required. Try values of `epsilon` between `0` and `1.0` and describe your findings. For roughly what values of `epsilon` are you able to correctly separate the two rings? 


```python
np.random.seed(420)
n = 1000
X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)
circ = spectral_clustering(X, epsilon = 0.525)
plt.scatter(X[:,0], X[:,1], c=circ)
```




    <matplotlib.collections.PathCollection at 0x7fc3fb58a110>




    
![png](output_41_1.png)
    



```python
np.random.seed(420)
n = 1000
X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)
circ = spectral_clustering(X, epsilon = 0.25)
plt.scatter(X[:,0], X[:,1], c=circ)
```




    <matplotlib.collections.PathCollection at 0x7fc3fb647550>




    
![png](output_42_1.png)
    



```python
np.random.seed(420)
n = 1000
X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)
circ = spectral_clustering(X, epsilon = 0.6)
plt.scatter(X[:,0], X[:,1], c=circ)
```




    <matplotlib.collections.PathCollection at 0x7fc3fb50bbd0>




    
![png](output_43_1.png)
    



```python
np.random.seed(420)
n = 1000
X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)
circ = spectral_clustering(X, epsilon = 0.2)
plt.scatter(X[:,0], X[:,1], c=circ)
```




    <matplotlib.collections.PathCollection at 0x7fc3f784e250>




    
![png](output_44_1.png)
    


We see here that our range is roughly (0.25, 0.525) after trial and error. We see if we try 0.6, our clustering isn't the best.

We also see if we lower our epsilon to 0.2, we cluster everything as one subgroup cluster!

Thus, we find that an $\epsilon \in (0.25, 0.525)$ allow us to accurately and correctly separate the two rings from one another!
